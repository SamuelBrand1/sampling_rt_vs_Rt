---
title: "Supporting information: Evaluating infection-generating processes for infectious disease situational awareness: Is the renewal process necessary?"
author:
  - name: Samuel P. C. Brand
    orcid: 0000-0003-0645-5367
    affiliations:
      - name: Center for Forecasting and Outbreak Analytics, Centers for Disease Control and Prevention, United States of America
  - name: Sam Abbott
    orcid: 0000-0001-8057-8037
    affiliations:
      - name: Centre for Mathematical Modelling of Infectious Diseases, London School of Hygiene & Tropical Medicine, London, United Kingdom
bibliography: references.bib
number-sections: true
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
---

```{julia}
#| echo: false
#| eval: false
using Pkg
index_location = @__DIR__()
Pkg.activate(index_location)
Pkg.resolve()
Pkg.instantiate()
Pkg.add(["CairoMakie", "JLD2", "DataFramesMeta", "DrWatson", "CSV"])

using DataFramesMeta, JLD2, CSV
```

## Simulated scenarios

### Scenarios considered but not implemented

- *Early outbreak*. Constant $R_t = R_0$ but for a short period.
- *Early outbreak with random effects*. As *Early outbreak* scenario but with white noise jitter on $R_t$.
- *Piecewise constant with large switches*: This scenario provides both sharp changes at the start of the timeseries and more gradual transitions towards the end. $R_t$ varies according to the following schedule:
  - 1.1 for two weeks
  - 2 for two weeks
  - 0.5 for two weeks
  - 1.5 for two weeks
  - 0.75 for two weeks
  - 1.1 for six weeks
  - sine curve centered at 1 with amplitude of 0.3 afterwards

## Model validation and prior specification

### Prior predictive modelling with default priors and transformations

As a first attempt, we used common priors for each latent process considered in this study: random walk, first order auto-regressive and differenced first-order auto-regressive. These priors were:

- The initial value parameter for all latent processes was:
$$
Z_0 \sim \text{Normal}(\text{mean} = 0, \text{std} = 0.25).
$$
- The standard deviation prior for all latent processes was:
$$
\sigma \sim \text{HalfNormal}(\text{mean} = 0.25).
$$
- The damping/auto-regression parameter for the auto-regressive latent processes was:
$$
\rho \sim \text{Beta}(a = 0.5, b = 0.5).
$$

For direct infection and renewal models the latent process represents a log-transformed epidemiological quantity, respectively: $Z_t = \log R_t$ and $Z_t = \log I_t$. The exponential growth rate modelling we identify the exponential growth rate with the latent process $Z_t = r_t$.

Using these priors we made prior predictive checks across our range of models. This was run with the pipeline script.

```bash
% julia pipeline/scripts/run_priorpred_pipeline.jl 1000
```

We noted that for a substantial number of the model configurations there were model predictive samples with such large numbers of infecteds that calls to `BigInt` caused `InexactError` exceptions. Rather than directly stop these exceptions we recorded the pattern of prior prediction failure so as to inform model improvement (Table \ref{tbl-prior-fail}).

```{julia}
#| output: false
#| eval: false
priorpred_dir = joinpath(@__DIR__(),"..", "pipeline/data/priorpredictive/")
priorpred_datafiles = readdir(priorpred_dir) |>
  fns -> filter(fn -> contains(fn, ".jld2"), fns) #filter for .jld2 files
priorpred_outcomes_df = DataFrame()
if !isfile(joinpath(index_location, "pass_fail_rdn1.csv"))
  priorpred_outcomes_df = mapreduce(vcat, priorpred_datafiles) do fn
    D = load(joinpath(priorpred_dir, fn))
    igp = D["inference_config"]["igp"]
    latent_model = D["inference_config"]["latent_model"]
    gi_mean = D["inference_config"]["gi_mean"]
    T1, T2 = split(D["inference_config"]["tspan"], "_")
    runsuccess = D["priorpredictive"] .== "Pass"
    df = DataFrame(
      infection_gen_proc = igp,
      latent_model = latent_model,
      gi_mean = gi_mean,
      T1 = T1,
      T2 = T2,
      T_diff = parse(Int, T2) - parse(Int, T1),
      runsuccess = runsuccess,
      )
  end
  CSV.write(joinpath(index_location, "pass_fail_rdn1.csv"), priorpred_outcomes_df)
else
  priorpred_outcomes_df = CSV.File(joinpath(index_location, "pass_fail_rdn1.csv")) |> DataFrame
end
```

```{julia}
#| label: tbl-prior-fail
#| tbl-cap: Number of prior predictive successes and fails from initial prior group grouped by infection generating process and latent model.
#| tbl-cap-location: bottom
#| eval: false
priorpred_outcomes_df |>
  df -> @groupby(df, :infection_gen_proc, :latent_model) |>
  gd -> @combine(gd, :n_success = sum(:runsuccess), :n_fail = sum(1 .- :runsuccess))
```

### Prior specification

This section provides more comprehensive information on the final prior specifications used in our analysis:

- Complete specification of all prior distributions
- Justification for prior choices
- Prior predictive check results for each model configuration
- Visualization of prior predictive distributions for key parameters

## Model implementation details

### MCMC diagnostics

This section presents detailed MCMC diagnostics for our model fitting:

- Convergence statistics (R-hat values) across model configurations
- Effective sample size statistics
- Analysis of divergent transitions and maximum tree depth warnings
- Posterior predictive checks for each model configuration

### Computational performance

This section details the computational performance of our models:

- Runtime comparisons across model configurations
- Memory usage statistics
- Scaling properties with dataset size
- Recommendations for computational resources needed for each model type

## Detailed evaluation results

### Performance by target measure

This section provides detailed evaluation results for each target measure:

- Comprehensive tables of performance metrics for each model and target measure
- Statistical analysis of performance differences
- Visualizations of performance patterns

### Performance by scenario

This section provides scenario-specific evaluation results:

- Detailed performance metrics for each model in each scenario
- Analysis of scenario-specific patterns in model performance
- Case studies highlighting particularly interesting behavior

### Sensitivity analyses

This section presents results from our sensitivity analyses:

- Impact of varying generation interval misspecification levels
- Sensitivity to initial conditions and parameter settings
- Effects of data quality and quantity on model performance

## Code availability

This section provides information on accessing and using our code:

- Repository location and access instructions
- Code structure and organization
- Instructions for reproducing our analyses
- Documentation of key functions and workflows

## References
